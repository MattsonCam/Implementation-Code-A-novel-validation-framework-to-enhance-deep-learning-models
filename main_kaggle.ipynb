{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom keras.models import Sequential, Model\nfrom keras.layers import LSTM, Dense, BatchNormalization, Reshape, Conv1D, Conv2D, \\\nMaxPooling2D, Concatenate, Dropout, Input, Flatten\nimport tensorflow as tf\nimport pandas as pd\nimport subprocess\nimport os\nos.chdir('../input/shared-files/utils')\nfrom fileHandling import getData\nfrom fileHandling import splitData\nfrom BLAS import create_dataset\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\npath_parent = os.path.dirname(os.getcwd())\nos.chdir(path_parent)\nos.chdir(path_parent)\nos.chdir('Data')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:19:43.056422Z","iopub.execute_input":"2022-04-26T02:19:43.056944Z","iopub.status.idle":"2022-04-26T02:19:48.257158Z","shell.execute_reply.started":"2022-04-26T02:19:43.056848Z","shell.execute_reply":"2022-04-26T02:19:48.256425Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def process_data(name):\n    SeriesName     = name\n    Strategy       = 'Differenced' # Differenced, Returns\n    Lag = 5\n    Dates      = ['2020-02-28',     '2020-06-01']\n    Series, Transformed_Series = getData(SeriesName + '.csv', Strategy)\n\n    Training,      Validation,      Testing      = splitData(Series,             Dates)\n    Training_diff, Validation_diff, Testing_diff = splitData(Transformed_Series, Dates)\n    # -Traditional Series\n    Validation      = pd.concat([Training.iloc[-Lag:],   Validation])\n    Testing         = pd.concat([Validation.iloc[-Lag:], Testing])\n    # -Differenced Series\n    Validation_diff = pd.concat([Training_diff.iloc[-Lag:],   Validation_diff])\n    Testing_diff    = pd.concat([Validation_diff.iloc[-Lag:], Testing_diff])\n\n    trainX, trainY = create_dataset(Training_diff,   Lag, SeriesName)\n    validX, validY = create_dataset(Validation_diff, Lag, SeriesName)\n    testX,  testY  = create_dataset(Testing_diff,    Lag, SeriesName)\n\n    scaler = StandardScaler()\n    trainX = np.expand_dims( scaler.fit_transform(trainX[:,:,0]), axis=-1)\n    validX = np.expand_dims( scaler.fit_transform(validX[:,:,0]), axis=-1)\n    testX  = np.expand_dims( scaler.fit_transform(testX[:,:,0]),  axis=-1)\n    \n    return trainX,trainY, validX,validY, testX,testY","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:19:48.258612Z","iopub.execute_input":"2022-04-26T02:19:48.258825Z","iopub.status.idle":"2022-04-26T02:19:48.272713Z","shell.execute_reply.started":"2022-04-26T02:19:48.258794Z","shell.execute_reply":"2022-04-26T02:19:48.271299Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"trainX_eth,trainY_eth, validX_eth,validY_eth, testX_eth,testY_eth = process_data('ETH')\ntrainX_btc,trainY_btc, validX_btc,validY_btc, testX_btc,testY_btc = process_data('BTC')\ntrainX_xrp,trainY_xrp, validX_xrp,validY_xrp, testX_xrp,testY_xrp = process_data('XRP')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:19:48.273838Z","iopub.execute_input":"2022-04-26T02:19:48.274403Z","iopub.status.idle":"2022-04-26T02:19:48.745465Z","shell.execute_reply.started":"2022-04-26T02:19:48.274370Z","shell.execute_reply":"2022-04-26T02:19:48.744766Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def lstmcomp(modinput, in_shape, k1 = 1, k2 = 2, k3 = 3, conv1dmaps = 16, \n             lay_padding = 'SAME', layer_act = 'relu'):\n    conv1 = Conv1D(conv1dmaps,k1,padding = lay_padding, activation=layer_act)(modinput)\n    conv2 = Conv1D(conv1dmaps,k2,padding = lay_padding, activation=layer_act)(modinput)\n    conv3 = Conv1D(conv1dmaps,k3,padding = lay_padding, activation=layer_act)(modinput)\n    dep = Concatenate()([conv1, conv2, conv3, modinput]) # Might just flatten this layer and feed it to\n    # the lstm\n    re1 = Reshape((in_shape, (conv1dmaps * 3) + 1, 1))(dep)\n    conv2d = Conv2D(1, (1,1), padding = lay_padding, activation=layer_act)(re1)\n    flat = Flatten()(conv2d)\n    re2 = Reshape((flat.shape[1],1))(flat)\n    # Did not add a dense layer here (might add it, but IDK if it is necessary)\n    lstm = LSTM(50)(re2)\n    #flat = Flatten()(conv2d)\n    #lstm = LSTM(50)(conv2d)\n    return lstm\n\ninshape = 5\ninput1 = Input(shape=(inshape, 1))\ninput2 = Input(shape=(inshape, 1))\ninput3 = Input(shape=(inshape, 1))\nout1 = lstmcomp(input1, inshape)\nout2 = lstmcomp(input2, inshape)\nout3 = lstmcomp(input3, inshape)\n\nconcat = Concatenate()([out1,out2,out3])\ndense1 = Dense(256, activation = 'relu')(concat)\nbatchnorm1 = BatchNormalization()(dense1)\ndrop1 = Dropout(3/10)(batchnorm1)\ndense2 = Dense(64)(drop1)\nbatchnorm2 = BatchNormalization()(dense2)\ndrop2 = Dropout(2/10)(batchnorm2)\n\nmodel = Model(inputs = [input1, input2, input3], outputs = drop2)\n#model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:19:48.747124Z","iopub.execute_input":"2022-04-26T02:19:48.747796Z","iopub.status.idle":"2022-04-26T02:19:51.891186Z","shell.execute_reply.started":"2022-04-26T02:19:48.747757Z","shell.execute_reply":"2022-04-26T02:19:51.890443Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#%%\n# Checkpoint\ncheckpoint = ModelCheckpoint(\"model/model.hdf5\", \n                              monitor        = 'val_loss', \n                              verbose        = 0, \n                              save_best_only = True, \n                              mode           = 'min')\n\n# Earlystopping\nearlystopping = EarlyStopping(monitor       = 'val_loss', \n                              mode          = 'min', \n                              verbose       = 1, \n                              patience      = 10)\n\n# Learning rate adjustment\n# lrs_scheduler = step_decay_schedule(initial_lr=1e-4, decay_factor=0.75, step_size=2)\nlrs_scheduler  = ReduceLROnPlateau(monitor     = 'val_loss', \n                                   factor      = 0.75,\n                                   patience    = 5)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:19:51.892645Z","iopub.execute_input":"2022-04-26T02:19:51.892882Z","iopub.status.idle":"2022-04-26T02:19:51.898817Z","shell.execute_reply.started":"2022-04-26T02:19:51.892850Z","shell.execute_reply":"2022-04-26T02:19:51.897887Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Need to finish this part still\npath_parent = os.path.dirname(os.getcwd())\nos.chdir(path_parent)\npath_parent = os.path.dirname(os.getcwd())\nos.chdir(path_parent)\npath_parent = os.path.dirname(os.getcwd())\nos.chdir(path_parent)\npath_parent = os.path.dirname(os.getcwd())\nos.chdir(path_parent)\n\nepochs         =  20\nbatch_size     =  8\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nscore = model.fit(x = [trainX_eth, trainX_btc, trainX_xrp], \n                  y = trainY_btc,\n                  epochs          = epochs, \n                  batch_size      = batch_size, \n                  callbacks       = [checkpoint, earlystopping, lrs_scheduler],\n                  verbose         = 1, \n                  validation_data = ([validX_eth, validX_btc, validX_xrp], validY_btc))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:19:51.900311Z","iopub.execute_input":"2022-04-26T02:19:51.900656Z","iopub.status.idle":"2022-04-26T02:21:09.876901Z","shell.execute_reply.started":"2022-04-26T02:19:51.900616Z","shell.execute_reply":"2022-04-26T02:21:09.876152Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"'''\nmodel.load_weights('model/model.hdf5')\n\ny_pred = model.predict(testX)[:,0]\n\n\n\n# Set DataFrame with 'Real' and 'Predicted' values\nPrices = pd.DataFrame([], columns=[SeriesName, 'Predict'])\n# Get real values\n\n# Reverse transformation\nif (Strategy == 'Differenced'):\n    Prices[ SeriesName ]    = Testing[SeriesName][Lag:]\n    Prices['Predict'] = Testing[SeriesName][Lag-1:-1].to_numpy() + y_pred\nelse:\n    Prices[ SeriesName ] = np.exp( Testing[SeriesName][Lag:] )\n    Prices['Predict']    = np.exp( Testing[SeriesName][Lag-1:-1].to_numpy() + y_pred )\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n'''\n# Regression performance\n#\nMAE, RMSE, MAPE, SMAPE, R2 = RegressionEvaluation( Prices )\n#\nprint('MAE   = %.3f' % MAE)\nprint('RMSE  = %.3f' % RMSE)\nprint('MAPE  = %.3f' % MAPE)\nprint('SMAPE = %.3f' % SMAPE)\nprint('R2    = %.3f' % R2)\nprint('\\n')\n\n\n\n# Classification performance\n#\nCM, Accuracy, AUC, F1, GM, Sen, Spe, PPV, NPV = ClassificationEvaluation( Prices )\n#\nprint('Accuracy  = %.2f%%' % (100*Accuracy))\nprint('AUC       = %.3f' % AUC)\nprint('F1        = %.3f' % F1)\nprint('GM        = %.3f' % GM)\nprint('Sen       = %.3f' % Sen)\nprint('Spe       = %.3f' % Spe)\nprint('Spe x Sen = %.3f' % (Sen*Spe))\nprint('\\n')\n\n\n# Confusion matrix\n#\nConfusionMatrixVisualize(CM)\n'''\n","metadata":{},"execution_count":null,"outputs":[]}]}