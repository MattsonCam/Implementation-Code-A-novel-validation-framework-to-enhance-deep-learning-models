{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131bb832-5a9d-4293-aa52-4c905a723616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, BatchNormalization, Reshape, Conv1D, Conv2D, \\\n",
    "MaxPooling2D, Concatenate, Dropout, Input, Flatten\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from utils.fileHandling import getData\n",
    "from utils.fileHandling import splitData\n",
    "from utils.BLAS import create_dataset\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45033fd0-2153-47da-b8d3-51026bcbdb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(name):\n",
    "    SeriesName     = name\n",
    "    Strategy       = 'Differenced' # Differenced, Returns\n",
    "    Lag = 5\n",
    "    Dates      = ['2020-02-28',     '2020-06-01']\n",
    "    Series, Transformed_Series = getData('Data//' + SeriesName + '.csv', Strategy)\n",
    "\n",
    "    Training,      Validation,      Testing      = splitData(Series,             Dates)\n",
    "    Training_diff, Validation_diff, Testing_diff = splitData(Transformed_Series, Dates)\n",
    "    # -Traditional Series\n",
    "    Validation      = pd.concat([Training.iloc[-Lag:],   Validation])\n",
    "    Testing         = pd.concat([Validation.iloc[-Lag:], Testing])\n",
    "    # -Differenced Series\n",
    "    Validation_diff = pd.concat([Training_diff.iloc[-Lag:],   Validation_diff])\n",
    "    Testing_diff    = pd.concat([Validation_diff.iloc[-Lag:], Testing_diff])\n",
    "\n",
    "    trainX, trainY = create_dataset(Training_diff,   Lag, SeriesName)\n",
    "    validX, validY = create_dataset(Validation_diff, Lag, SeriesName)\n",
    "    testX,  testY  = create_dataset(Testing_diff,    Lag, SeriesName)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    trainX = np.expand_dims( scaler.fit_transform(trainX[:,:,0]), axis=-1)\n",
    "    validX = np.expand_dims( scaler.fit_transform(validX[:,:,0]), axis=-1)\n",
    "    testX  = np.expand_dims( scaler.fit_transform(testX[:,:,0]),  axis=-1)\n",
    "    \n",
    "    return trainX,trainY, validX,validY, testX,testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f9b1f-5568-45bb-8b25-ce81c9869dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_eth,trainY_eth, validX_eth,validY_eth, testX_eth,testY_eth = process_data('ETH')\n",
    "trainX_btc,trainY_btc, validX_btc,validY_btc, testX_btc,testY_btc = process_data('BTC')\n",
    "trainX_xrp,trainY_xrp, validX_xrp,validY_xrp, testX_xrp,testY_xrp = process_data('XRP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e7455-ecd5-4d6f-9c28-7b769ffe2592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstmcomp(modinput, in_shape, k1 = 1, k2 = 2, k3 = 3, conv1dmaps = 16, \n",
    "             lay_padding = 'SAME', layer_act = 'relu'):\n",
    "    conv1 = Conv1D(conv1dmaps,k1,padding = lay_padding, activation=layer_act)(modinput)\n",
    "    conv2 = Conv1D(conv1dmaps,k2,padding = lay_padding, activation=layer_act)(modinput)\n",
    "    conv3 = Conv1D(conv1dmaps,k3,padding = lay_padding, activation=layer_act)(modinput)\n",
    "    dep = Concatenate()([conv1, conv2, conv3, modinput]) # Might just flatten this layer and feed it to\n",
    "    # the lstm\n",
    "    re1 = Reshape((in_shape, (conv1dmaps * 3) + 1, 1))(dep)\n",
    "    conv2d = Conv2D(1, (1,1), padding = lay_padding, activation=layer_act)(re1)\n",
    "    flat = Flatten()(conv2d)\n",
    "    re2 = Reshape((flat.shape[1],1))(flat)\n",
    "    # Did not add a dense layer here (might add it, but IDK if it is necessary)\n",
    "    lstm = LSTM(50)(re2)\n",
    "    #flat = Flatten()(conv2d)\n",
    "    #lstm = LSTM(50)(conv2d)\n",
    "    return lstm\n",
    "\n",
    "inshape = 5\n",
    "input1 = Input(shape=(inshape, 1))\n",
    "input2 = Input(shape=(inshape, 1))\n",
    "input3 = Input(shape=(inshape, 1))\n",
    "out1 = lstmcomp(input1, inshape)\n",
    "out2 = lstmcomp(input2, inshape)\n",
    "out3 = lstmcomp(input3, inshape)\n",
    "\n",
    "concat = Concatenate()([out1,out2,out3])\n",
    "dense1 = Dense(256, activation = 'relu')(concat)\n",
    "batchnorm1 = BatchNormalization()(dense1)\n",
    "drop1 = Dropout(3/10)(batchnorm1)\n",
    "dense2 = Dense(64)(drop1)\n",
    "batchnorm2 = BatchNormalization()(dense2)\n",
    "drop2 = Dropout(2/10)(batchnorm2)\n",
    "\n",
    "model = Model(inputs = [input1, input2, input3], outputs = drop2)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb3d01e-841f-456c-806d-948a16767218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Checkpoint\n",
    "checkpoint = ModelCheckpoint(\"model/model.hdf5\", \n",
    "                              monitor        = 'val_loss', \n",
    "                              verbose        = 0, \n",
    "                              save_best_only = True, \n",
    "                              mode           = 'min')\n",
    "\n",
    "# Earlystopping\n",
    "earlystopping = EarlyStopping(monitor       = 'val_loss', \n",
    "                              mode          = 'min', \n",
    "                              verbose       = 1, \n",
    "                              patience      = 10)\n",
    "\n",
    "# Learning rate adjustment\n",
    "# lrs_scheduler = step_decay_schedule(initial_lr=1e-4, decay_factor=0.75, step_size=2)\n",
    "lrs_scheduler  = ReduceLROnPlateau(monitor     = 'val_loss', \n",
    "                                   factor      = 0.75,\n",
    "                                   patience    = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a71372-f8ae-4956-81bb-b72712e6d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to finish this part still\n",
    "epochs         =  20\n",
    "batch_size     =  8\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "score = model.fit(x = [trainX_eth, trainX_btc, trainX_xrp], \n",
    "                  y = trainY_btc,\n",
    "                  epochs          = epochs, \n",
    "                  batch_size      = batch_size, \n",
    "                  callbacks       = [checkpoint, earlystopping, lrs_scheduler],\n",
    "                  verbose         = 1, \n",
    "                  validation_data = (validX_btc, validY_btc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
