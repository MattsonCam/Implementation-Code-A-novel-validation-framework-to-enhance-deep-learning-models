{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom keras.models import Sequential, Model\nfrom keras.layers import LSTM, Dense, BatchNormalization, Reshape, Conv1D, Conv2D, \\\nMaxPooling2D, AveragePooling1D, Concatenate, Dropout, Input, Flatten\nimport tensorflow as tf\nimport pandas as pd\nimport subprocess\nimport matplotlib.pyplot as plt\nimport os\nos.chdir('../input/shared-files/utils')\nfrom Evaluation import RegressionEvaluation, smape\nfrom fileHandling import getData\nfrom fileHandling import splitData\nfrom BLAS import create_dataset\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\npath_parent = os.path.dirname(os.getcwd())\nos.chdir(path_parent)\nos.chdir(path_parent)\nos.chdir('Data')","metadata":{"execution":{"iopub.status.busy":"2022-05-02T20:36:26.759942Z","iopub.execute_input":"2022-05-02T20:36:26.760548Z","iopub.status.idle":"2022-05-02T20:36:32.852433Z","shell.execute_reply.started":"2022-05-02T20:36:26.760443Z","shell.execute_reply":"2022-05-02T20:36:32.851578Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def process_data(name, strat = 'Differenced', lg = 7):\n    SeriesName     = name\n    Strategy       = strat # Differenced, Returns\n    Lag = lg\n    Dates      = ['2020-03-01',     '2020-05-31'] # These are the dates of the validation set\n    Series, Transformed_Series = getData(SeriesName + '.csv', Strategy) # Transforms the series based on the difference between values\n    # The transformed series starts from January 2nd 2017, and the orther series starts at January 1st\n    \n    # These split the data according to the datetime. The training set  is any data before the start date of the validation data\n    # The validation data is anything between the dates above, and the test data is any date after the training data.\n    Training,      Validation,      Testing      = splitData(Series,             Dates)\n    Training_diff, Validation_diff, Testing_diff = splitData(Transformed_Series, Dates)\n\n    # Takes the last lag (eg. 7) days of the training data and concatenates it with the validation set to create the validation set.\n    # -Traditional Series\n    Validation      = pd.concat([Training.iloc[-Lag:],   Validation])\n    Testing         = pd.concat([Validation.iloc[-Lag:], Testing])\n    # Takes the last lag (eg. 7) days of the validation data and concatenates it with the testing set to create the testing set.\n    # -Differenced Series\n    Validation_diff = pd.concat([Training_diff.iloc[-Lag:],   Validation_diff])\n    Testing_diff    = pd.concat([Validation_diff.iloc[-Lag:], Testing_diff])\n    \n    # Uses a sliding window of size lage (eg. 7) to create the X data. Then takes the lag+1 (eg. 8) data point to create the Y value\n    trainX, trainY = create_dataset(Training_diff,   Lag, SeriesName)\n    validX, validY = create_dataset(Validation_diff, Lag, SeriesName)\n    testX,  testY  = create_dataset(Testing_diff,    Lag, SeriesName)\n\n    # Changed the validX and the testX to transform, because we won't have enought info to fit in real life.\n    scaler = StandardScaler()\n    trainX = np.expand_dims( scaler.fit_transform(trainX[:,:,0]), axis=-1)\n    validX = np.expand_dims( scaler.fit_transform(validX[:,:,0]), axis=-1)\n    testX  = np.expand_dims( scaler.fit_transform(testX[:,:,0]),  axis=-1)\n    \n    if (strat == 'Differenced'):\n        test_data = Testing_diff\n    \n    else:\n        test_data = Testing\n    \n    return trainX,trainY, validX,validY, testX,testY, test_data","metadata":{"execution":{"iopub.status.busy":"2022-05-02T20:36:32.856166Z","iopub.execute_input":"2022-05-02T20:36:32.856813Z","iopub.status.idle":"2022-05-02T20:36:32.869787Z","shell.execute_reply.started":"2022-05-02T20:36:32.856780Z","shell.execute_reply":"2022-05-02T20:36:32.869140Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"SeriesName     = 'ETH'\nStrategy       = 'Returns' # Differenced, Returns\nLag = 7\n\ntrainX_eth,trainY_eth, validX_eth,validY_eth, testX_eth,testY_eth, Testing_eth = process_data('ETH', Strategy, Lag)\ntrainX_btc,trainY_btc, validX_btc,validY_btc, testX_btc,testY_btc, Testing_btc = process_data('BTC', Strategy, Lag)\ntrainX_xrp,trainY_xrp, validX_xrp,validY_xrp, testX_xrp,testY_xrp, Testing_xrp = process_data('XRP', Strategy, Lag)\n\nif (SeriesName == 'ETH'):\n    trainX,trainY, validX,validY, testX,testY, Testing = trainX_eth,trainY_eth, validX_eth,validY_eth, testX_eth,testY_eth, Testing_eth\n\nelif (SeriesName == 'BTC'):\n    trainX,trainY, validX,validY, testX,testY, Testing = trainX_btc,trainY_btc, validX_btc,validY_btc, testX_btc,testY_btc, Testing_btc\n\nelse:\n    trainX,trainY, validX,validY, testX,testY, Testing = trainX_xrp,trainY_xrp, validX_xrp,validY_xrp, testX_xrp,testY_xrp, Testing_xrp","metadata":{"execution":{"iopub.status.busy":"2022-05-02T20:36:32.871003Z","iopub.execute_input":"2022-05-02T20:36:32.871284Z","iopub.status.idle":"2022-05-02T20:36:33.360422Z","shell.execute_reply.started":"2022-05-02T20:36:32.871246Z","shell.execute_reply":"2022-05-02T20:36:33.359684Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def lstmcomp(modinput, in_shape, k1 = 1, k2 = 2, k3 = 3, conv1dmaps = 16, \n             lay_padding = 'SAME', layer_act = 'relu'):\n    \n    conv = Conv1D(16,2,padding = lay_padding, activation=layer_act)(modinput) #should we add padding?\n    pool = AveragePooling1D(2)(conv)\n    flat = Flatten()(pool)\n    re2 = Reshape((flat.shape[1],1))(flat)\n    lstm = LSTM(50)(re2)\n\n    '''\n    conv1 = Conv1D(conv1dmaps,k1,padding = lay_padding, activation=layer_act)(modinput)\n    conv2 = Conv1D(conv1dmaps,k2,padding = lay_padding, activation=layer_act)(modinput)\n    conv3 = Conv1D(conv1dmaps,k3,padding = lay_padding, activation=layer_act)(modinput)\n    dep = Concatenate()([conv1, conv2, conv3, modinput]) # Might just flatten this layer and feed it to\n    # the lstm\n    re1 = Reshape((in_shape, (conv1dmaps * 3) + 1, 1))(dep)\n    conv2d = Conv2D(1, (1,1), padding = lay_padding, activation=layer_act)(re1)\n    flat = Flatten()(conv2d)\n    re2 = Reshape((flat.shape[1],1))(flat)\n    # Did not add a dense layer here (might add it, but IDK if it is necessary)\n    lstm = LSTM(50)(re2)\n    #flat = Flatten()(conv2d)\n    #lstm = LSTM(50)(conv2d)\n    '''\n    return lstm\n\ninshape1 = trainX.shape[1]\ninshape2 = trainX.shape[2]\ninput1 = Input(shape=(inshape1, inshape2))\ninput2 = Input(shape=(inshape1, inshape2))\ninput3 = Input(shape=(inshape1, inshape2))\nout1 = lstmcomp(input1, inshape1)\nout2 = lstmcomp(input2, inshape1)\nout3 = lstmcomp(input3, inshape1)\n\nconcat = Concatenate()([out1,out2,out3])\ndense1 = Dense(256, activation = 'relu')(concat)\nbatchnorm1 = BatchNormalization()(dense1)\ndrop1 = Dropout(3/10)(batchnorm1)\ndense2 = Dense(64, activation = 'relu')(drop1)\nbatchnorm2 = BatchNormalization()(dense2)\ndrop2 = Dropout(2/10)(batchnorm2)\nfinal = Dense(1)(drop2)\n\nmodel = Model(inputs = [input1, input2, input3], outputs = final)\nmodel.summary()\n'''\ninshape = 5\ninput1 = Input(shape=(inshape, 1))\nout1 = lstmcomp(input1, inshape)\ndense1 = Dense(256, activation = 'relu')(out1)\nfinal = Dense(1)(dense1)\nmodel = Model(input1, final)\n'''","metadata":{"execution":{"iopub.status.busy":"2022-05-02T20:36:33.362668Z","iopub.execute_input":"2022-05-02T20:36:33.362933Z","iopub.status.idle":"2022-05-02T20:36:36.498751Z","shell.execute_reply.started":"2022-05-02T20:36:33.362899Z","shell.execute_reply":"2022-05-02T20:36:36.498092Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#%%\n# Checkpoint\ncheckpoint = ModelCheckpoint(\"model/model.hdf5\", \n                              monitor        = 'val_loss', \n                              verbose        = 0, \n                              save_best_only = True, \n                              mode           = 'min')\n\n# Earlystopping\nearlystopping = EarlyStopping(monitor       = 'val_loss', \n                              mode          = 'min', \n                              verbose       = 1, \n                              patience      = 10)\n\n# Learning rate adjustment\n# lrs_scheduler = step_decay_schedule(initial_lr=1e-4, decay_factor=0.75, step_size=2)\nlrs_scheduler  = ReduceLROnPlateau(monitor     = 'val_loss', \n                                   factor      = 0.75,\n                                   patience    = 5)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T20:36:36.499935Z","iopub.execute_input":"2022-05-02T20:36:36.500261Z","iopub.status.idle":"2022-05-02T20:36:36.505717Z","shell.execute_reply.started":"2022-05-02T20:36:36.500225Z","shell.execute_reply":"2022-05-02T20:36:36.505045Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Need to finish this part still\npath_parent = os.path.dirname(os.getcwd())\nos.chdir(path_parent)\npath_parent = os.path.dirname(os.getcwd())\nos.chdir(path_parent)\npath_parent = os.path.dirname(os.getcwd())\nos.chdir(path_parent)\npath_parent = os.path.dirname(os.getcwd())\nos.chdir(path_parent)\n\nepochs         =  20\nbatch_size     =  8\nmodel.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.RMSprop())\n\nscore = model.fit(x = [trainX_btc, trainX_eth, trainX_xrp], \n                  y = trainY,\n                  epochs          = epochs, \n                  batch_size      = batch_size, \n                  callbacks       = [checkpoint, earlystopping, lrs_scheduler],\n                  verbose         = 1, \n                  validation_data = ([validX_btc, validX_eth, validX_xrp], validY))\n'''\nscore = model.fit(trainX_btc, \n                  y = trainY_btc,\n                  epochs          = epochs, \n                  batch_size      = batch_size, \n                  callbacks       = [checkpoint, earlystopping, lrs_scheduler],\n                  verbose         = 1, \n                  validation_data = (validX_btc, validY_btc))\n'''","metadata":{"execution":{"iopub.status.busy":"2022-05-02T20:36:36.506863Z","iopub.execute_input":"2022-05-02T20:36:36.507160Z","iopub.status.idle":"2022-05-02T20:37:28.195317Z","shell.execute_reply.started":"2022-05-02T20:36:36.507106Z","shell.execute_reply":"2022-05-02T20:37:28.194670Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model.load_weights('model/model.hdf5')","metadata":{"execution":{"iopub.status.busy":"2022-05-02T20:37:28.198894Z","iopub.execute_input":"2022-05-02T20:37:28.202218Z","iopub.status.idle":"2022-05-02T20:37:28.242416Z","shell.execute_reply.started":"2022-05-02T20:37:28.202180Z","shell.execute_reply":"2022-05-02T20:37:28.241507Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"'''\nfrom sklearn import metrics\nimport math\n\ny_pred = model.predict([testX_btc, testX_eth, testX_xrp])\ny_pred = np.squeeze(y_pred)\n\ndef smape(A, F):\n    return ( 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)) ) )\n\ndef Regeval2( Y, Pred ):\n    MAE   = metrics.mean_absolute_error(Y, Pred)\n    RMSE  = math.sqrt(metrics.mean_squared_error(Y, Pred))\n    MAPE  = np.mean(np.abs((Y - Pred) / Y)) * 100.0\n    SMAPE = smape(Y, Pred)\n    R2    = metrics.r2_score(Y, Pred)\n    return (MAE, RMSE) #SMAPE, R2)\n'''","metadata":{"execution":{"iopub.status.busy":"2022-05-02T20:37:28.243947Z","iopub.execute_input":"2022-05-02T20:37:28.244423Z","iopub.status.idle":"2022-05-02T20:37:28.250865Z","shell.execute_reply.started":"2022-05-02T20:37:28.244381Z","shell.execute_reply":"2022-05-02T20:37:28.249998Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict([testX_btc, testX_eth, testX_xrp])[:,0]\n\n# Set DataFrame with 'Real' and 'Predicted' values\nPrices = pd.DataFrame([], columns=[SeriesName, 'Predict'])\n# Get real values\n\n# Reverse transformation\nif (Strategy == 'Differenced'):\n    Prices[ SeriesName ]    = Testing[SeriesName][Lag:]\n    Prices['Predict'] = Testing[SeriesName][Lag-1:-1 ].to_numpy() + y_pred\nelse:\n    Prices[ SeriesName ] = np.exp( Testing[SeriesName][Lag:] )\n    Prices['Predict']    = np.exp( Testing[SeriesName][Lag-1:-1].to_numpy() + y_pred )\n    \nax = Prices.plot( figsize=(15,4), linewidth=2, marker='*')\nax.set_ylabel('Price')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T20:37:28.252558Z","iopub.execute_input":"2022-05-02T20:37:28.253026Z","iopub.status.idle":"2022-05-02T20:37:29.598217Z","shell.execute_reply.started":"2022-05-02T20:37:28.252989Z","shell.execute_reply":"2022-05-02T20:37:29.597424Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Regression performance\n#\nMAE, RMSE, MAPE, SMAPE, R2 = RegressionEvaluation( Prices )\n#\nprint('MAE   = %.3f' % MAE)\nprint('RMSE  = %.3f' % RMSE)\nprint('MAPE  = %.3f' % MAPE)\nprint('SMAPE = %.3f' % SMAPE)\nprint('R2    = %.3f' % R2)\nprint('\\n')\n\n\n'''\n# Classification performance\n#\nCM, Accuracy, AUC, F1, GM, Sen, Spe, PPV, NPV = ClassificationEvaluation( Prices )\n#\nprint('Accuracy  = %.2f%%' % (100*Accuracy))\nprint('AUC       = %.3f' % AUC)\nprint('F1        = %.3f' % F1)\nprint('GM        = %.3f' % GM)\nprint('Sen       = %.3f' % Sen)\nprint('Spe       = %.3f' % Spe)\nprint('Spe x Sen = %.3f' % (Sen*Spe))\nprint('\\n')\n\n\n# Confusion matrix\n#\nConfusionMatrixVisualize(CM)\n'''","metadata":{"execution":{"iopub.status.busy":"2022-05-02T20:37:29.600340Z","iopub.execute_input":"2022-05-02T20:37:29.601090Z","iopub.status.idle":"2022-05-02T20:37:29.614851Z","shell.execute_reply.started":"2022-05-02T20:37:29.601031Z","shell.execute_reply":"2022-05-02T20:37:29.614022Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}